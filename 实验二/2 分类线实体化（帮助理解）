import matplotlib.pyplot as plt
import numpy as np
NumDataPerClass = 200

def PercentCorrect(Inputs, targets, weights):
    N = len(targets)
    nCorrect = 0
    for n in range(N):
        OneInput = Inputs[n,:]
        if (targets[n] * np.dot(OneInput, weights) > 0):
            nCorrect +=1
    return 100*nCorrect/N

# Two-class problem, distinct means, equal covariance matrices
m1 = [[2.5, 2.5]]
m2 = [[10, 10]]
C = [[2, 1], [1, 2]]
# Set up the data by generating isotropic Guassians and
# rotating them accordingly
A = np.linalg.cholesky(C)
U1 = np.random.randn(NumDataPerClass,2)
X1 = U1 @ A.T + m1
U2 = np.random.randn(NumDataPerClass,2)
X2 = U2 @ A.T + m2
X = np.concatenate((X1, X2), axis=0)

O = np.ones((2*NumDataPerClass, 1))
X = np.append(X, O, axis=1)

fig, ax = plt.subplots(figsize=(5,5))
ax.scatter(X1[:,0], X1[:,1], c="r", s=4)
ax.scatter(X2[:,0], X2[:,1], c="b", s=4)
ax.scatter(X[:,0], X[:,1], c="c", s=4)
ax.set_xlim(-4, 15)
ax.set_ylim(-4, 15)

plt.show()

labelPos = np.ones(NumDataPerClass)
labelNeg = -1.0 * np.ones(NumDataPerClass)
y = np.concatenate((labelPos, labelNeg))
#----------------------------------------------------------------
rIndex = np.random.permutation(2*NumDataPerClass)
Xr = X[rIndex,]
yr = y[rIndex]
# Training and test sets (half half)
X_train = Xr[0:NumDataPerClass]
y_train = yr[0:NumDataPerClass]
X_test = Xr[NumDataPerClass:2*NumDataPerClass]
y_test = yr[NumDataPerClass:2*NumDataPerClass]
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
Ntrain = NumDataPerClass;
Ntest = NumDataPerClass;
#-------------------------------------------------------------------
# Perceptron learning loop
# Random initialization of weights
#w = np.random.randn(2)
w = np.random.randn(3)
print(w)


# What is the performance with the initial random weights?
print('Initial Percentage Correct: %6.2f' %(PercentCorrect(X_train, y_train, w)))
# Fixed number of iterations (think of better stopping criterion)
MaxIter=1000
# Learning rate (change this to see convergence changing)
alpha = 0.01
# Space to save answers for plotting
P_train = np.zeros(MaxIter)
P_test = np.zeros(MaxIter)
# Main Loop
for iter in range(MaxIter):
    # Select a data item at random
    r = np.floor(np.random.rand()*Ntrain).astype(int)
    x = X_train[r,:]
    # If it is misclassified, update weights
    if (y_train[r] * np.dot(x, w) < 0):
        w += alpha * y_train[r] * x
    # Evaluate trainign and test performances for plotting
    P_train[iter] = PercentCorrect(X_train, y_train, w);
    P_test[iter] = PercentCorrect(X_test, y_test, w);
print('Percentage Correct After Training: %6.2f %6.2f'%(PercentCorrect(X_train, y_train, w), PercentCorrect(X_test, y_test, w)))

fig, ax = plt.subplots(figsize=(5,5))
ax.scatter(X1[:,0], X1[:,1], c="r", s=4)
ax.scatter(X2[:,0], X2[:,1], c="b", s=4)
ax.scatter(X[:,0], X[:,1], c="c", s=4)
ax.set_xlim(-4, 15)
ax.set_ylim(-4, 15)
a=np.linspace(-5,15)
b=0-(w[0]*a+w[2])/w[1]
plt.plot(a,b)
plt.show()

fig, ax = plt.subplots(figsize=(6,4))
ax.plot(range(MaxIter), P_train, 'b', label = "Training")
ax.plot(range(MaxIter), P_test, 'r', label = "Test")
ax.grid(True)
ax.legend()
ax.set_title('Perceptron Learning')
ax.set_ylabel('Training and Test Accuracies', fontsize=14)
ax.set_xlabel('Iteration', fontsize=14)
plt.savefig('learningCurves.png')


#---------------------------------------------------------------
#ax.scatter(X_test[:,0], Y_test[:,1], c="b", s=4)
#ax.scatter(X[:,0], X[:,1], c="c", s=4)
#ax.set_xlim(-4, 16)
#ax.set_ylim(-4, 16)

plt.show()
